{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Purpose \n",
    "\n",
    "\n",
    "The purpose of this notebook is mainly to build the model, train it and evaluate it with a live feed through the webcam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up basic utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'hand_landmarks'\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Little data preprocessing \n",
    "\n",
    "## Accessing dataset that has been collected in previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand_landmarks\\class_0\n",
      "hand_landmarks\\class_1\n",
      "hand_landmarks\\class_2\n"
     ]
    }
   ],
   "source": [
    "for label in range(3):\n",
    "    class_dir = os.path.join(save_dir, 'class_{}'.format(label))\n",
    "    print(class_dir)\n",
    "    for file_name in os.listdir(class_dir):\n",
    "        file_path = os.path.join(class_dir, file_name)\n",
    "        landmark = np.load(file_path)\n",
    "        labels.append(label)\n",
    "        data.append(landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model with live feed by accessing the webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hand gesture labels\n",
    "labels = ['Rock', 'Paper', 'Scissors']\n",
    "#landmarks = []\n",
    "# Initialize the MediaPipe Hand module\n",
    "mp_hands_2 = mp.solutions.hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    # Read frames from the webcam\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert the frame to RGB for input to MediaPipe\n",
    "    #frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False \n",
    "    results = mp_hands_2.process(image)\n",
    "\n",
    "    image.flags.writeable = True \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Detect hand landmarks using MediaPipe\n",
    "    #results = mp_hands_2.process(frame_rgb)\n",
    "\n",
    "    # Check if hand landmarks are detected\n",
    "     \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(image, hand_landmarks\n",
    "                                      , mp_hands.HAND_CONNECTIONS)\n",
    "        # Extract the landmark coordinates\n",
    "            image_height, image_width, _ = image.shape\n",
    "\n",
    "            landmark_list = []\n",
    "\n",
    "            for landmark in hand_landmarks.landmark: \n",
    "                landmark_x = landmark.x * image_width\n",
    "                landmark_y = landmark.y * image_height\n",
    "\n",
    "                landmark_list.extend([landmark_x, landmark_y, landmark.z])\n",
    "            #landmarks.append(landmark_list)\n",
    "            \n",
    "            landmark_array = np.array(landmark_list)\n",
    "            landmark_array = np.expand_dims(landmark_array, axis=0)\n",
    "            # Perform prediction\n",
    "            print(len(landmark_list))\n",
    "            prediction = model.predict(landmark_array)\n",
    "            predicted_class = np.argmax(prediction)\n",
    "            gesture_label = labels[predicted_class]\n",
    "\n",
    "            # Draw the predicted gesture label on the frame\n",
    "            cv2.putText(frame, gesture_label, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Hand Gesture Detection', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organizing the dataset for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(zip(data, labels))\n",
    "random.shuffle(dataset)\n",
    "data_, labels_ = zip(*dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.27902260e+02,  2.99752808e+02, -4.39823538e-07,  4.01977921e+02,\n",
       "        2.46460333e+02, -1.21167889e-02,  3.81016426e+02,  1.97910333e+02,\n",
       "       -3.44027840e-02,  3.55619659e+02,  1.67270265e+02, -5.61827831e-02,\n",
       "        3.50769043e+02,  1.58199692e+02, -7.25529045e-02,  4.49759483e+02,\n",
       "        1.58735876e+02, -3.88408229e-02,  3.76228218e+02,  1.44802752e+02,\n",
       "       -7.24378750e-02,  3.69927979e+02,  1.72313075e+02, -8.39188099e-02,\n",
       "        3.87392654e+02,  1.82521219e+02, -8.94800723e-02,  4.53388138e+02,\n",
       "        1.84885211e+02, -5.17292842e-02,  3.62608986e+02,  1.73327923e+02,\n",
       "       -8.54131877e-02,  3.65556984e+02,  1.99183702e+02, -8.29801708e-02,\n",
       "        3.85987015e+02,  2.04944401e+02, -7.86534101e-02,  4.46063805e+02,\n",
       "        2.16021638e+02, -6.65851533e-02,  3.60319557e+02,  2.11438165e+02,\n",
       "       -9.46287364e-02,  3.67483978e+02,  2.32011738e+02, -7.88755789e-02,\n",
       "        3.88800621e+02,  2.33573785e+02, -6.60151243e-02,  4.32827950e+02,\n",
       "        2.49051905e+02, -8.26511383e-02,  3.66489410e+02,  2.43569727e+02,\n",
       "       -9.96452048e-02,  3.73522263e+02,  2.57425203e+02, -9.05029401e-02,\n",
       "        3.93499374e+02,  2.59360542e+02, -8.09914470e-02])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data_)\n",
    "labels = np.array(labels_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_landmarks = data[:-200]\n",
    "train_labels = labels[:-200]\n",
    "test_landmarks = data[-200:]\n",
    "test_labels = labels[-200:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Attempt to use LSTM\n",
    "\n",
    "\n",
    "# timesteps = 10  # You can try different values here\n",
    "\n",
    "# # Reshape the input data to add the timesteps dimension\n",
    "# num_samples = train_landmarks.shape[0]  # Total number of samples in the training set\n",
    "# input_data_reshaped = train_landmarks.reshape(num_samples, timesteps, 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_shape = (10, 63)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture building and training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\ssffff\\anaconda3\\envs\\rps\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\ssffff\\anaconda3\\envs\\rps\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\ssffff\\anaconda3\\envs\\rps\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\ssffff\\anaconda3\\envs\\rps\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\ssffff\\anaconda3\\envs\\rps\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\ssffff\\anaconda3\\envs\\rps\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_5\" is incompatible with the layer: expected shape=(None, 10, 63), found shape=(None, 63)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 30\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[0;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     28\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 30\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_landmarks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[0;32m     33\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_landmarks, test_labels)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rps\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file_6jxy85l.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\ssffff\\anaconda3\\envs\\rps\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\ssffff\\anaconda3\\envs\\rps\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\ssffff\\anaconda3\\envs\\rps\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\ssffff\\anaconda3\\envs\\rps\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\ssffff\\anaconda3\\envs\\rps\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\ssffff\\anaconda3\\envs\\rps\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_5\" is incompatible with the layer: expected shape=(None, 10, 63), found shape=(None, 63)\n"
     ]
    }
   ],
   "source": [
    "num_landmarks = 21  # Number of hand landmarks detected by MediaPipe\n",
    "num_classes = 3  # Number of hand gesture classes (rock, paper, scissors)\n",
    "\n",
    "\n",
    "# Current model saved as model2/ in the directory.\n",
    "# NB: model1/ was trained with fewer dataset than model2/ but with the same architecture.\n",
    "\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu',\n",
    "                          input_shape=(num_landmarks * 3,)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# Attempted Model for getting better performance\n",
    "\n",
    "# model = tf.keras.models.Sequential([\n",
    "#         tf.keras.layers.LSTM(128, return_sequences=True, input_shape=input_shape),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.LSTM(64),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(128, activation='relu'),\n",
    "#         tf.keras.layers.Dropout(0.5),\n",
    "#         tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "#     ]) \n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_landmarks, train_labels, epochs=25)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_landmarks, test_labels)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"./model2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
